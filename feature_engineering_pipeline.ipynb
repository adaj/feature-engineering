{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering Pipeline com Apache Airflow\n",
    "\n",
    "## Introdução\n",
    "\n",
    "Bem-vindos a este tutorial sobre Feature Engineering e Apache Airflow! Neste notebook, vamos explorar como construir um pipeline de engenharia de features utilizando dados climáticos como exemplo prático.\n",
    "\n",
    "### Objetivos de Aprendizado\n",
    "\n",
    "Ao final deste tutorial, você será capaz de:\n",
    "- Compreender os conceitos fundamentais de feature engineering\n",
    "- Aplicar técnicas de feature engineering usando scikit-learn\n",
    "- Entender como estruturar um pipeline ETL com Apache Airflow\n",
    "- Implementar transformações de dados como polynomial features e one-hot encoding\n",
    "- Conhecer o básico sobre feature stores com Feast (opcional)\n",
    "\n",
    "### Estrutura do Tutorial\n",
    "\n",
    "1. **Conceitos Básicos de Feature Engineering**\n",
    "2. **Técnicas de Feature Engineering com scikit-learn**\n",
    "3. **Implementação de um Pipeline ETL com Apache Airflow**\n",
    "4. **Desafio: Integração com Feast Feature Store**\n",
    "\n",
    "Vamos começar!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Conceitos Básicos de Feature Engineering\n",
    "\n",
    "### O que é Feature Engineering?\n",
    "\n",
    "Feature Engineering é o processo de transformar dados brutos em features (características) que representam melhor o problema subjacente para os algoritmos de machine learning, resultando em melhor desempenho do modelo.\n",
    "\n",
    "É considerada uma das etapas mais importantes e trabalhosas no desenvolvimento de modelos de machine learning, pois a qualidade das features tem impacto direto na qualidade do modelo final.\n",
    "\n",
    "### Por que Feature Engineering é importante?\n",
    "\n",
    "1. **Melhora o desempenho do modelo**: Features bem projetadas podem capturar relações importantes nos dados que algoritmos simples podem não detectar.\n",
    "\n",
    "2. **Reduz a complexidade do modelo**: Com features melhores, você pode usar modelos mais simples e ainda obter bons resultados.\n",
    "\n",
    "3. **Incorpora conhecimento de domínio**: Permite que especialistas no assunto contribuam com seu conhecimento para o processo de machine learning.\n",
    "\n",
    "4. **Lida com dados problemáticos**: Ajuda a tratar valores ausentes, outliers e outras anomalias nos dados.\n",
    "\n",
    "### Processo de Feature Engineering\n",
    "\n",
    "O processo de feature engineering geralmente envolve as seguintes etapas:\n",
    "\n",
    "1. **Exploração e Compreensão dos Dados**: Entender a natureza dos dados e identificar padrões.\n",
    "\n",
    "2. **Limpeza de Dados**: Tratar valores ausentes, outliers e erros.\n",
    "\n",
    "3. **Transformação de Features**: Aplicar operações matemáticas para criar novas features (ex: polinomiais).\n",
    "\n",
    "4. **Codificação de Variáveis Categóricas**: Converter variáveis categóricas em numéricas (ex: one-hot encoding).\n",
    "\n",
    "5. **Seleção de Features**: Escolher as features mais relevantes para o modelo.\n",
    "\n",
    "6. **Normalização/Padronização**: Ajustar a escala das features para melhorar o desempenho do modelo.\n",
    "\n",
    "No nosso exemplo com dados climáticos, vamos aplicar várias dessas técnicas para criar um conjunto de features útil para análise e modelagem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Técnicas de Feature Engineering com scikit-learn\n",
    "\n",
    "Vamos explorar algumas das técnicas de feature engineering mais comuns usando a biblioteca scikit-learn. Para nosso exemplo, usaremos dados climáticos da API OpenWeatherMap.\n",
    "\n",
    "### Configuração Inicial\n",
    "\n",
    "Primeiro, vamos importar as bibliotecas necessárias:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import requests\n",
    "import json\n",
    "from sklearn.preprocessing import PolynomialFeatures, OneHotEncoder, StandardScaler\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "# Configurações de visualização\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette('viridis')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtenção dos Dados Climáticos\n",
    "\n",
    "Para este tutorial, vamos simular a obtenção de dados da API OpenWeatherMap. Em um ambiente real, você precisaria de uma chave de API válida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulação de dados climáticos para várias cidades\n",
    "# Em um ambiente real, você faria uma chamada à API OpenWeatherMap\n",
    "\n",
    "def simulate_weather_data(cities):\n",
    "    \"\"\"Simula dados climáticos para uma lista de cidades.\"\"\"\n",
    "    np.random.seed(42)  # Para reprodutibilidade\n",
    "    weather_conditions = ['Clear', 'Clouds', 'Rain', 'Snow', 'Thunderstorm', 'Drizzle', 'Mist']\n",
    "    \n",
    "    data = []\n",
    "    for city in cities:\n",
    "        for day in range(10):  # Simula 10 dias de dados\n",
    "            data.append({\n",
    "                'city': city,\n",
    "                'date': pd.Timestamp('2023-10-01') + pd.Timedelta(days=day),\n",
    "                'temp': np.random.uniform(5, 35),  # Temperatura em Celsius\n",
    "                'feels_like': np.random.uniform(3, 37),  # Sensação térmica\n",
    "                'temp_min': np.random.uniform(2, 30),  # Temperatura mínima\n",
    "                'temp_max': np.random.uniform(10, 40),  # Temperatura máxima\n",
    "                'pressure': np.random.uniform(990, 1030),  # Pressão atmosférica\n",
    "                'humidity': np.random.uniform(30, 100),  # Umidade\n",
    "                'wind_speed': np.random.uniform(0, 20),  # Velocidade do vento\n",
    "                'wind_deg': np.random.uniform(0, 360),  # Direção do vento\n",
    "                'clouds': np.random.uniform(0, 100),  # Cobertura de nuvens\n",
    "                'weather_condition': np.random.choice(weather_conditions)  # Condição climática\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# Lista de cidades para simular dados\n",
    "cities = ['New York', 'London', 'Tokyo', 'Sydney', 'Rio de Janeiro', 'Paris', 'Moscow', 'Cairo']\n",
    "\n",
    "# Gerar dados simulados\n",
    "weather_df = simulate_weather_data(cities)\n",
    "\n",
    "# Exibir as primeiras linhas do DataFrame\n",
    "weather_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explorando os Dados\n",
    "\n",
    "Antes de aplicar técnicas de feature engineering, é importante entender os dados com os quais estamos trabalhando."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Informações básicas sobre o DataFrame\n",
    "print(\"Informações do DataFrame:\")\n",
    "weather_df.info()\n",
    "\n",
    "print(\"\\nEstatísticas descritivas:\")\n",
    "weather_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar a distribuição de algumas variáveis numéricas\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "sns.histplot(weather_df['temp'], kde=True, ax=axes[0, 0])\n",
    "axes[0, 0].set_title('Distribuição de Temperatura')\n",
    "\n",
    "sns.histplot(weather_df['humidity'], kde=True, ax=axes[0, 1])\n",
    "axes[0, 1].set_title('Distribuição de Umidade')\n",
    "\n",
    "sns.histplot(weather_df['wind_speed'], kde=True, ax=axes[1, 0])\n",
    "axes[1, 0].set_title('Distribuição de Velocidade do Vento')\n",
    "\n",
    "sns.histplot(weather_df['pressure'], kde=True, ax=axes[1, 1])\n",
    "axes[1, 1].set_title('Distribuição de Pressão Atmosférica')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar a contagem de condições climáticas\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(y='weather_condition', data=weather_df)\n",
    "plt.title('Contagem de Condições Climáticas')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Criação de Features Relacionadas ao Tempo\n",
    "\n",
    "Vamos extrair informações temporais da coluna de data, como dia da semana, mês, estação do ano, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extrair features temporais\n",
    "weather_df['day_of_week'] = weather_df['date'].dt.dayofweek\n",
    "weather_df['is_weekend'] = weather_df['day_of_week'].apply(lambda x: 1 if x >= 5 else 0)  # 5=Sábado, 6=Domingo\n",
    "weather_df['month'] = weather_df['date'].dt.month\n",
    "weather_df['day'] = weather_df['date'].dt.day\n",
    "\n",
    "# Definir estação do ano (simplificado para hemisfério norte)\n",
    "def get_season(month):\n",
    "    if month in [12, 1, 2]:\n",
    "        return 'Winter'\n",
    "    elif month in [3, 4, 5]:\n",
    "        return 'Spring'\n",
    "    elif month in [6, 7, 8]:\n",
    "        return 'Summer'\n",
    "    else:  # 9, 10, 11\n",
    "        return 'Fall'\n",
    "\n",
    "weather_df['season'] = weather_df['month'].apply(get_season)\n",
    "\n",
    "# Exibir as novas colunas\n",
    "weather_df[['date', 'day_of_week', 'is_weekend', 'month', 'day', 'season']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aplicando Polynomial Features\n",
    "\n",
    "As polynomial features são úteis para capturar relações não-lineares nos dados. Vamos aplicá-las às variáveis numéricas relacionadas ao clima."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecionar colunas numéricas para aplicar polynomial features\n",
    "numerical_cols = ['temp', 'humidity', 'wind_speed', 'pressure']\n",
    "X_numerical = weather_df[numerical_cols]\n",
    "\n",
    "# Aplicar polynomial features (grau 2)\n",
    "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "poly_features = poly.fit_transform(X_numerical)\n",
    "\n",
    "# Obter os nomes das features polinomiais\n",
    "poly_feature_names = poly.get_feature_names_out(numerical_cols)\n",
    "\n",
    "# Criar DataFrame com as features polinomiais\n",
    "poly_df = pd.DataFrame(poly_features, columns=poly_feature_names)\n",
    "\n",
    "# Exibir as primeiras linhas do DataFrame com features polinomiais\n",
    "poly_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aplicando One-Hot Encoding\n",
    "\n",
    "O one-hot encoding é usado para converter variáveis categóricas em formato numérico, criando uma coluna binária para cada categoria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecionar colunas categóricas para aplicar one-hot encoding\n",
    "categorical_cols = ['weather_condition', 'season', 'city']\n",
    "X_categorical = weather_df[categorical_cols]\n",
    "\n",
    "# Aplicar one-hot encoding\n",
    "encoder = OneHotEncoder(sparse=False, drop='first')  # drop='first' para evitar multicolinearidade\n",
    "encoded_features = encoder.fit_transform(X_categorical)\n",
    "\n",
    "# Obter os nomes das features codificadas\n",
    "encoded_feature_names = encoder.get_feature_names_out(categorical_cols)\n",
    "\n",
    "# Criar DataFrame com as features codificadas\n",
    "encoded_df = pd.DataFrame(encoded_features, columns=encoded_feature_names)\n",
    "\n",
    "# Exibir as primeiras linhas do DataFrame com features codificadas\n",
    "encoded_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Criando Features de Interação\n",
    "\n",
    "As features de interação podem capturar relações importantes entre diferentes variáveis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar algumas features de interação manualmente\n",
    "weather_df['temp_humidity_ratio'] = weather_df['temp'] / weather_df['humidity']\n",
    "weather_df['heat_index'] = weather_df['temp'] * (1 + 0.01 * weather_df['humidity'])  # Simplificação do índice de calor\n",
    "weather_df['wind_chill'] = 13.12 + 0.6215 * weather_df['temp'] - 11.37 * (weather_df['wind_speed'] ** 0.16) + 0.3965 * weather_df['temp'] * (weather_df['wind_speed'] ** 0.16)  # Fórmula simplificada\n",
    "\n",
    "# Exibir as novas features\n",
    "weather_df[['temp', 'humidity', 'wind_speed', 'temp_humidity_ratio', 'heat_index', 'wind_chill']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combinando Todas as Features\n",
    "\n",
    "Agora vamos combinar todas as features que criamos em um único DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecionar features originais que queremos manter\n",
    "original_features = ['city', 'date', 'temp', 'humidity', 'wind_speed', 'pressure', 'weather_condition']\n",
    "original_df = weather_df[original_features].copy()\n",
    "\n",
    "# Selecionar features temporais\n",
    "time_features = ['day_of_week', 'is_weekend', 'month', 'day', 'season']\n",
    "time_df = weather_df[time_features].copy()\n",
    "\n",
    "# Selecionar features de interação\n",
    "interaction_features = ['temp_humidity_ratio', 'heat_index', 'wind_chill']\n",
    "interaction_df = weather_df[interaction_features].copy()\n",
    "\n",
    "# Combinar todos os DataFrames\n",
    "# Nota: Não incluímos as features polinomiais e one-hot encoding diretamente para evitar um DataFrame muito grande\n",
    "combined_df = pd.concat([original_df, time_df, interaction_df], axis=1)\n",
    "\n",
    "# Exibir as primeiras linhas do DataFrame combinado\n",
    "combined_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seleção de Features\n",
    "\n",
    "A seleção de features é importante para reduzir a dimensionalidade e melhorar o desempenho do modelo. Vamos usar o método de limiar de variância para selecionar features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparar dados para seleção de features\n",
    "# Primeiro, precisamos converter todas as colunas para numéricas\n",
    "# Vamos usar apenas as colunas numéricas e as que já codificamos\n",
    "\n",
    "# Selecionar colunas numéricas do DataFrame combinado\n",
    "numeric_columns = combined_df.select_dtypes(include=['number']).columns\n",
    "X_for_selection = combined_df[numeric_columns].copy()\n",
    "\n",
    "# Normalizar os dados para que a variância seja comparável\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_for_selection)\n",
    "X_scaled_df = pd.DataFrame(X_scaled, columns=X_for_selection.columns)\n",
    "\n",
    "# Aplicar seleção por limiar de variância\n",
    "selector = VarianceThreshold(threshold=0.01)  # Remover features com variância < 0.01\n",
    "X_selected = selector.fit_transform(X_scaled_df)\n",
    "\n",
    "# Obter os nomes das features selecionadas\n",
    "selected_features = X_for_selection.columns[selector.get_support()]\n",
    "print(f\"Features originais: {len(X_for_selection.columns)}\")\n",
    "print(f\"Features selecionadas: {len(selected_features)}\")\n",
    "print(\"\\nFeatures selecionadas:\")\n",
    "print(selected_features.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar DataFrame final com as features selecionadas\n",
    "final_df = combined_df[['city', 'date']].copy()  # Manter colunas de identificação\n",
    "for feature in selected_features:\n",
    "    if feature in combined_df.columns:\n",
    "        final_df[feature] = combined_df[feature]\n",
    "\n",
    "# Exibir as primeiras linhas do DataFrame final\n",
    "final_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Implementação de um Pipeline ETL com Apache Airflow\n",
    "\n",
    "Agora que entendemos as técnicas de feature engineering, vamos ver como implementar um pipeline ETL completo usando Apache Airflow.\n",
    "\n",
    "### O que é Apache Airflow?\n",
    "\n",
    "Apache Airflow é uma plataforma de código aberto para criar, agendar e monitorar fluxos de trabalho programaticamente. Com o Airflow, você pode definir seus pipelines de dados como código, tornando-os mais fáceis de manter, versionar e escalar.\n",
    "\n",
    "### Componentes Principais do Airflow\n",
    "\n",
    "- **DAG (Directed Acyclic Graph)**: Representa um fluxo de trabalho como um grafo acíclico direcionado de tarefas.\n",
    "- **Operators**: Definem o que realmente é executado em cada tarefa (ex: PythonOperator, BashOperator).\n",
    "- **Tasks**: Instâncias parametrizadas de operadores.\n",
    "- **Task Dependencies**: Definem a ordem de execução das tarefas.\n",
    "\n",
    "### Estrutura do nosso Pipeline ETL\n",
    "\n",
    "Nosso pipeline ETL para feature engineering seguirá estas etapas:\n",
    "\n",
    "1. **Extract**: Obter dados climáticos da API OpenWeatherMap.\n",
    "2. **Transform**: Aplicar técnicas de feature engineering aos dados.\n",
    "3. **Load**: Armazenar os resultados em um banco de dados MongoDB.\n",
    "\n",
    "Vamos examinar o código do DAG que implementa este pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Este é o código do DAG do Airflow\n",
    "# Em um ambiente Jupyter, não podemos executar diretamente o Airflow\n",
    "# Este código é apenas para fins educacionais\n",
    "\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "from airflow import DAG\n",
    "from airflow.operators.python import PythonOperator\n",
    "from airflow.providers.http.operators.http import SimpleHttpOperator\n",
    "from airflow.models import Variable\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.preprocessing import PolynomialFeatures, OneHotEncoder\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "import numpy as np\n",
    "from pymongo import MongoClient\n",
    "\n",
    "# Define functions outside the DAG block\n",
    "\n",
    "def parse_weather_response(**context):\n",
    "    # Parse the HTTP response into a DataFrame.\n",
    "    response = context[\"task_instance\"].xcom_pull(task_ids=\"fetch_weather\")\n",
    "    data = json.loads(response)\n",
    "    weather_data = {\n",
    "        \"temp\": data[\"main\"][\"temp\"],\n",
    "        \"humidity\": data[\"main\"][\"humidity\"],\n",
    "        \"wind_speed\": data[\"wind\"][\"speed\"],\n",
    "        \"weather_condition\": data[\"weather\"][0][\"main\"]\n",
    "    }\n",
    "    df = pd.DataFrame([weather_data])\n",
    "    context[\"task_instance\"].xcom_push(key=\"weather_df\", value=df.to_json())\n",
    "\n",
    "def apply_polynomial_features(**context):\n",
    "    # Apply polynomial features to numerical columns.\n",
    "    df_json = context[\"task_instance\"].xcom_pull(task_ids=\"parse_weather\", key=\"weather_df\")\n",
    "    df = pd.read_json(df_json)\n",
    "    poly_degree = context[\"dag_run\"].conf.get(\"poly_degree\", 2)\n",
    "    numerical_cols = [\"temp\", \"humidity\", \"wind_speed\"]\n",
    "    poly = PolynomialFeatures(degree=poly_degree, include_bias=False)\n",
    "    poly_features = poly.fit_transform(df[numerical_cols])\n",
    "    feature_names = poly.get_feature_names_out(numerical_cols)\n",
    "    df_poly = pd.DataFrame(poly_features, columns=feature_names)\n",
    "    context[\"task_instance\"].xcom_push(key=\"poly_features\", value=df_poly.to_json())\n",
    "\n",
    "def apply_one_hot_encoding(**context):\n",
    "    # Apply one-hot encoding to categorical columns. \n",
    "    df_json = context[\"task_instance\"].xcom_pull(task_ids=\"parse_weather\", key=\"weather_df\")\n",
    "    df = pd.read_json(df_json)\n",
    "    categorical_cols = [\"weather_condition\"]\n",
    "    encoder = OneHotEncoder(sparse=False, drop=\"first\")\n",
    "    encoded_features = encoder.fit_transform(df[categorical_cols])\n",
    "    feature_names = encoder.get_feature_names_out(categorical_cols)\n",
    "    df_encoded = pd.DataFrame(encoded_features, columns=feature_names)\n",
    "    context[\"task_instance\"].xcom_push(key=\"encoded_features\", value=df_encoded.to_json())\n",
    "\n",
    "def combine_features(**context):\n",
    "    # Combine polynomial and encoded features into a single DataFrame.\n",
    "    poly_json = context[\"task_instance\"].xcom_pull(task_ids=\"apply_polynomial\", key=\"poly_features\")\n",
    "    encoded_json = context[\"task_instance\"].xcom_pull(task_ids=\"apply_encoding\", key=\"encoded_features\")\n",
    "    df_poly = pd.read_json(poly_json)\n",
    "    df_encoded = pd.read_json(encoded_json)\n",
    "    df_combined = pd.concat([df_poly, df_encoded], axis=1)\n",
    "    context[\"task_instance\"].xcom_push(key=\"combined_features\", value=df_combined.to_json())\n",
    "\n",
    "def feature_selection(**context):\n",
    "    # Perform variance-based feature selection.\n",
    "    df_json = context[\"task_instance\"].xcom_pull(task_ids=\"combine_features\", key=\"combined_features\")\n",
    "    df = pd.read_json(df_json)\n",
    "    selector = VarianceThreshold(threshold=0.01)  # Remove features with variance < 0.01\n",
    "    selected_features = selector.fit_transform(df)\n",
    "    feature_names = df.columns[selector.get_support()]\n",
    "    df_selected = pd.DataFrame(selected_features, columns=feature_names)\n",
    "    context[\"task_instance\"].xcom_push(key=\"selected_features\", value=df_selected.to_json())\n",
    "\n",
    "def load_to_mongodb(**context):\n",
    "    # Load the transformed data into MongoDB.\n",
    "    df_json = context[\"task_instance\"].xcom_pull(task_ids=\"feature_selection\", key=\"selected_features\")\n",
    "    df = pd.read_json(df_json)\n",
    "    client = MongoClient(\"mongodb://localhost:27017/\")  # Update if using MongoDB Atlas\n",
    "    db = client[\"weather_db\"]\n",
    "    collection = db[\"daily_weather\"]\n",
    "    collection.insert_many(df.to_dict(\"records\"))\n",
    "    \n",
    "    # Optional: For advanced users, you could integrate Feast (a feature store) here.\n",
    "    # Example (commented out):\n",
    "    # from feast import Client\n",
    "    # feast_client = Client(core_url=\"your_feast_core_url\")\n",
    "    # feast_client.apply_entity(\"location\", ...)\n",
    "    # feast_client.apply_feature_view(weather_features, ...)\n",
    "    # feast_client.ingest(\"weather_features\", df)\n",
    "\n",
    "# Define the DAG and operators inside the \"with\" block\n",
    "with DAG(\n",
    "    \"daily_weather_etl\",\n",
    "    default_args={\n",
    "        \"owner\": \"airflow\",\n",
    "        \"start_date\": datetime(2023, 10, 1),\n",
    "        \"retries\": 1,\n",
    "        \"retry_delay\": timedelta(minutes=5),\n",
    "    },\n",
    "    schedule_interval=\"@daily\",\n",
    "    catchup=False,\n",
    "    params={\"city\": \"New York\", \"poly_degree\": 2}  # Default parameters\n",
    ") as dag:\n",
    "\n",
    "    # Task 1: Fetch weather data from OpenWeatherMap API\n",
    "    fetch_weather = SimpleHttpOperator(\n",
    "        task_id=\"fetch_weather\",\n",
    "        http_conn_id=\"openweather_conn\",  # Connection set in Airflow UI\n",
    "        endpoint=\"data/2.5/weather?q={{ dag_run.conf[\\'city\\'] }}&appid={{ var.value.weather_api_key }}&units=metric\",\n",
    "        method=\"GET\",\n",
    "        xcom_push=True,  # Push response to XCom\n",
    "    )\n",
    "\n",
    "    # Task 2: Parse the weather response\n",
    "    parse_task = PythonOperator(\n",
    "        task_id=\"parse_weather\",\n",
    "        python_callable=parse_weather_response,\n",
    "        provide_context=True,\n",
    "    )\n",
    "\n",
    "    # Task 3: Apply polynomial features\n",
    "    poly_task = PythonOperator(\n",
    "        task_id=\"apply_polynomial\",\n",
    "        python_callable=apply_polynomial_features,\n",
    "        provide_context=True,\n",
    "    )\n",
    "\n",
    "    # Task 4: Apply one-hot encoding\n",
    "    encode_task = PythonOperator(\n",
    "        task_id=\"apply_encoding\",\n",
    "        python_callable=apply_one_hot_encoding,\n",
    "        provide_context=True,\n",
    "    )\n",
    "\n",
    "    # Task 5: Combine features\n",
    "    combine_task = PythonOperator(\n",
    "        task_id=\"combine_features\",\n",
    "        python_callable=combine_features,\n",
    "        provide_context=True,\n",
    "    )\n",
    "\n",
    "    # Task 6: Perform feature selection\n",
    "    selection_task = PythonOperator(\n",
    "        task_id=\"feature_selection\",\n",
    "        python_callable=feature_selection,\n",
    "        provide_context=True,\n",
    "    )\n",
    "\n",
    "    # Task 7: Load data into MongoDB\n",
    "    load_task = PythonOperator(\n",
    "        task_id=\"load_to_db\",\n",
    "        python_callable=load_to_mongodb,\n",
    "        provide_context=True,\n",
    "    )\n",
    "\n",
    "    # Set task dependencies\n",
    "    fetch_weather >> parse_task\n",
    "    parse_task >> [poly_task, encode_task]  # Parallel execution\n",
    "    [poly_task, encode_task] >> combine_task\n",
    "    combine_task >> selection_task >> load_task\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explicação do Pipeline ETL\n",
    "\n",
    "Vamos analisar o pipeline ETL implementado no código acima:\n",
    "\n",
    "#### 1. Extração (Extract)\n",
    "- Utilizamos o `SimpleHttpOperator` para fazer uma requisição à API OpenWeatherMap.\n",
    "- Os parâmetros da requisição (cidade, chave de API) são configurados no Airflow.\n",
    "- A resposta da API é armazenada usando o sistema XCom do Airflow para compartilhamento entre tarefas.\n",
    "\n",
    "#### 2. Transformação (Transform)\n",
    "- **Parsing**: Convertemos a resposta JSON em um DataFrame pandas.\n",
    "- **Feature Engineering**:\n",
    "  - Aplicamos polynomial features às variáveis numéricas.\n",
    "  - Aplicamos one-hot encoding às variáveis categóricas.\n",
    "  - Combinamos as features transformadas.\n",
    "  - Realizamos seleção de features baseada em variância.\n",
    "\n",
    "#### 3. Carregamento (Load)\n",
    "- Armazenamos as features processadas em um banco de dados MongoDB.\n",
    "- Opcionalmente, poderíamos integrar com o Feast para gerenciamento de features.\n",
    "\n",
    "### Fluxo de Execução do DAG\n",
    "\n",
    "O fluxo de execução do DAG é definido pelas dependências entre as tarefas:\n",
    "\n",
    "```\n",
    "fetch_weather >> parse_task\n",
    "parse_task >> [poly_task, encode_task]  # Execução paralela\n",
    "[poly_task, encode_task] >> combine_task\n",
    "combine_task >> selection_task >> load_task\n",
    "```\n",
    "\n",
    "Este fluxo pode ser visualizado como:\n",
    "\n",
    "```\n",
    "                                  ┌─────────────┐\n",
    "                                  │ parse_task  │\n",
    "                                  └──────┬──────┘\n",
    "                                         │\n",
    "                                         ▼\n",
    "┌──────────────┐                ┌────────────────┐                ┌────────────────┐\n",
    "│ fetch_weather ├───────────────► poly_task      ├────────────────►                │\n",
    "└──────────────┘                └────────────────┘                │                │\n",
    "                                                                  │ combine_task   ├───► selection_task ───► load_task\n",
    "                                ┌────────────────┐                │                │\n",
    "                                │ encode_task    ├────────────────►                │\n",
    "                                └────────────────┘                └────────────────┘\n",
    "```\n",
    "\n",
    "### Vantagens do Apache Airflow para Feature Engineering\n",
    "\n",
    "1. **Automação**: Executa o pipeline automaticamente conforme o agendamento definido.\n",
    "2. **Monitoramento**: Fornece uma interface visual para monitorar a execução do pipeline.\n",
    "3. **Escalabilidade**: Pode lidar com grandes volumes de dados e pipelines complexos.\n",
    "4. **Flexibilidade**: Permite definir fluxos de trabalho complexos com execução paralela e dependências.\n",
    "5. **Recuperação de Falhas**: Oferece mecanismos para lidar com falhas e retentativas.\n",
    "6. **Versionamento**: O pipeline é definido como código, facilitando o versionamento e a colaboração."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Desafio: Integração com Feast Feature Store\n",
    "\n",
    "Como desafio opcional, vamos explorar como integrar nosso pipeline de feature engineering com o Feast, um feature store de código aberto.\n",
    "\n",
    "### O que é um Feature Store?\n",
    "\n",
    "Um feature store é um sistema especializado para armazenar, gerenciar e servir features para modelos de machine learning. Ele resolve problemas comuns em pipelines de ML, como:\n",
    "\n",
    "1. **Consistência entre treinamento e inferência**: Garante que as mesmas transformações sejam aplicadas aos dados de treinamento e produção.\n",
    "2. **Reutilização de features**: Permite que features sejam compartilhadas entre diferentes modelos e equipes.\n",
    "3. **Monitoramento**: Facilita o monitoramento de drift e qualidade das features.\n",
    "4. **Governança**: Fornece linhagem e documentação para features.\n",
    "\n",
    "### Introdução ao Feast\n",
    "\n",
    "Feast (Feature Store) é uma ferramenta de código aberto para gerenciar, servir e descobrir features para machine learning. Ele fornece:\n",
    "\n",
    "- Um registro centralizado de definições de features\n",
    "- Armazenamento online e offline para features\n",
    "- APIs para recuperação de features para treinamento e inferência\n",
    "- Integração com várias fontes de dados e plataformas de ML\n",
    "\n",
    "### Como Integrar Feast ao Nosso Pipeline\n",
    "\n",
    "Vamos ver como poderíamos integrar o Feast ao nosso pipeline de feature engineering:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Este é um exemplo conceitual de como integrar Feast ao nosso pipeline\n",
    "# Em um ambiente real, você precisaria instalar e configurar o Feast\n",
    "\n",
    "\n",
    "# Primeiro, definimos as entidades e features no Feast\n",
    "# Arquivo: feature_repo/feature_definitions.py\n",
    "\n",
    "from datetime import timedelta\n",
    "from feast import Entity, Feature, FeatureView, ValueType\n",
    "from feast.data_source import FileSource\n",
    "\n",
    "# Define a entidade para nossos dados climáticos\n",
    "location = Entity(name=\"location\", value_type=ValueType.STRING, description=\"Localização da medição climática\")\n",
    "\n",
    "# Define a fonte de dados para as features climáticas\n",
    "weather_source = FileSource(\n",
    "    path=\"/path/to/weather_features.parquet\",  # Caminho para o arquivo de features\n",
    "    event_timestamp_column=\"date\",\n",
    ")\n",
    "\n",
    "# Define a view de features para os dados climáticos\n",
    "weather_features_view = FeatureView(\n",
    "    name=\"weather_features\",\n",
    "    entities=[location],\n",
    "    ttl=timedelta(days=3),  # Time-to-live para as features\n",
    "    features=[\n",
    "        Feature(name=\"temp\", dtype=ValueType.FLOAT),\n",
    "        Feature(name=\"humidity\", dtype=ValueType.FLOAT),\n",
    "        Feature(name=\"wind_speed\", dtype=ValueType.FLOAT),\n",
    "        Feature(name=\"temp_squared\", dtype=ValueType.FLOAT),\n",
    "        Feature(name=\"humidity_squared\", dtype=ValueType.FLOAT),\n",
    "        Feature(name=\"temp_humidity\", dtype=ValueType.FLOAT),\n",
    "        Feature(name=\"is_rainy\", dtype=ValueType.INT32),\n",
    "        Feature(name=\"is_sunny\", dtype=ValueType.INT32),\n",
    "    ],\n",
    "    online=True,\n",
    "    input=weather_source,\n",
    "    tags={\"team\": \"weather_analytics\"},\n",
    ")\n",
    "\n",
    "# Agora, modificamos a função load_to_mongodb no nosso DAG para incluir a integração com Feast\n",
    "\n",
    "def load_to_feast(**context):\n",
    "    # Load the transformed data into Feast feature store.\n",
    "    from feast import Client\n",
    "    import pandas as pd\n",
    "    from datetime import datetime\n",
    "    \n",
    "    # Obter as features processadas\n",
    "    df_json = context[\"task_instance\"].xcom_pull(task_ids=\"feature_selection\", key=\"selected_features\")\n",
    "    df = pd.read_json(df_json)\n",
    "    \n",
    "    # Adicionar colunas necessárias para o Feast\n",
    "    df[\"location\"] = context[\"dag_run\"].conf.get(\"city\", \"New York\")  # Entidade\n",
    "    df[\"date\"] = datetime.now()  # Timestamp do evento\n",
    "    \n",
    "    # Salvar o DataFrame em um formato que o Feast possa ler\n",
    "    parquet_path = \"/path/to/weather_features.parquet\"\n",
    "    df.to_parquet(parquet_path)\n",
    "    \n",
    "    # Conectar ao Feast e aplicar as definições de features\n",
    "    feast_client = Client(repo_path=\"/path/to/feature_repo\")\n",
    "    feast_client.apply()  # Aplica as definições de features\n",
    "    \n",
    "    # Ingerir os dados no feature store\n",
    "    feast_client.materialize_incremental(datetime.now() - timedelta(days=1), datetime.now())\n",
    "    \n",
    "    return \"Features loaded to Feast successfully\"\n",
    "\n",
    "# Adicionar a tarefa ao DAG\n",
    "load_feast_task = PythonOperator(\n",
    "    task_id=\"load_to_feast\",\n",
    "    python_callable=load_to_feast,\n",
    "    provide_context=True,\n",
    ")\n",
    "\n",
    "# Modificar as dependências\n",
    "selection_task >> load_feast_task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benefícios da Integração com Feast\n",
    "\n",
    "Integrar nosso pipeline de feature engineering com o Feast traz vários benefícios:\n",
    "\n",
    "1. **Consistência**: Garante que as mesmas transformações sejam aplicadas aos dados de treinamento e produção.\n",
    "2. **Reutilização**: Permite que as features sejam facilmente reutilizadas em diferentes modelos.\n",
    "3. **Escalabilidade**: Facilita o gerenciamento de features em projetos de grande escala.\n",
    "4. **Monitoramento**: Fornece ferramentas para monitorar a qualidade e o drift das features.\n",
    "5. **Documentação**: Mantém um registro centralizado de metadados sobre as features.\n",
    "\n",
    "### Desafio para os Alunos\n",
    "\n",
    "Como desafio, tente implementar a integração com Feast seguindo estes passos:\n",
    "\n",
    "1. Instale o Feast usando `pip install feast`.\n",
    "2. Crie um repositório de features com `feast init feature_repo`.\n",
    "3. Defina suas entidades e features no arquivo `feature_definitions.py`.\n",
    "4. Modifique o pipeline Airflow para incluir a integração com Feast.\n",
    "5. Teste a recuperação de features para treinamento e inferência.\n",
    "\n",
    "Este desafio é uma excelente oportunidade para explorar ferramentas avançadas de MLOps e entender como o feature engineering se encaixa em um fluxo de trabalho de machine learning de produção."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusão\n",
    "\n",
    "Neste tutorial, exploramos o processo completo de feature engineering, desde a compreensão dos conceitos básicos até a implementação de um pipeline ETL automatizado com Apache Airflow.\n",
    "\n",
    "### Recapitulando o que aprendemos:\n",
    "\n",
    "1. **Conceitos de Feature Engineering**:\n",
    "   - Importância da feature engineering para modelos de machine learning\n",
    "   - Processo de transformação de dados brutos em features úteis\n",
    "\n",
    "2. **Técnicas de Feature Engineering com scikit-learn**:\n",
    "   - Criação de features temporais\n",
    "   - Aplicação de polynomial features\n",
    "   - One-hot encoding para variáveis categóricas\n",
    "   - Criação de features de interação\n",
    "   - Seleção de features baseada em variância\n",
    "\n",
    "3. **Implementação de Pipeline ETL com Apache Airflow**:\n",
    "   - Estrutura de um DAG no Airflow\n",
    "   - Extração de dados de APIs\n",
    "   - Transformação de dados com técnicas de feature engineering\n",
    "   - Carregamento de dados em bancos de dados\n",
    "\n",
    "4. **Integração com Feast Feature Store (Desafio)**:\n",
    "   - Conceito de feature store\n",
    "   - Benefícios da utilização de um feature store\n",
    "   - Implementação básica com Feast\n",
    "\n",
    "### Próximos Passos\n",
    "\n",
    "Para continuar seu aprendizado em feature engineering e pipelines de dados:\n",
    "\n",
    "1. **Experimente com diferentes técnicas de feature engineering**:\n",
    "   - Transformações não-lineares\n",
    "   - Técnicas de redução de dimensionalidade (PCA, t-SNE)\n",
    "   - Métodos de seleção de features baseados em modelos\n",
    "\n",
    "2. **Explore recursos avançados do Apache Airflow**:\n",
    "   - Sensores e gatilhos\n",
    "   - Branching e condicionais\n",
    "   - Paralelismo e pools\n",
    "\n",
    "3. **Aprofunde-se em MLOps**:\n",
    "   - Implementação completa com Feast\n",
    "   - Monitoramento de modelos e features\n",
    "   - Integração com plataformas de ML\n",
    "\n",
    "Lembre-se: a feature engineering é tanto uma ciência quanto uma arte. A prática e a experimentação são essenciais para desenvolver boas intuições sobre quais transformações funcionarão melhor para seus dados específicos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercício Prático\n",
    "\n",
    "Para consolidar o conhecimento adquirido, tente o seguinte exercício:\n",
    "\n",
    "1. Expanda o conjunto de dados climáticos simulado para incluir mais variáveis (ex: pressão atmosférica, visibilidade, ponto de orvalho).\n",
    "2. Crie novas features baseadas em conhecimento de domínio sobre meteorologia.\n",
    "3. Implemente técnicas adicionais de feature engineering não abordadas no tutorial.\n",
    "4. Avalie a importância das features criadas usando um modelo simples (ex: RandomForest).\n",
    "5. Bônus: Tente implementar a integração com Feast conforme descrito no desafio.\n",
    "\n",
    "Compartilhe seus resultados e insights com a turma!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
